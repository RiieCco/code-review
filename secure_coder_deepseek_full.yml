# ============================================================
# Base model: DeepSeek Coder Instruct
# ============================================================
base_model: deepseek-ai/deepseek-coder-6.7b-instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

# ============================================================
# QLoRA Adapter
# ============================================================
adapter: qlora
load_in_4bit: true
strict: false

lora_r: 64
lora_alpha: 16
lora_dropout: 0.05

lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

bnb_4bit_quant_type: nf4
bnb_4bit_compute_dtype: bfloat16
bnb_4bit_use_double_quant: true

# ============================================================
# Combined Datasets (HF + Local)
# ============================================================
datasets:

  # -------------------------
  #  PUBLIC DATASETS (HF)
  # -------------------------

  - path: JulioCesarCargnin/CVEfixes
    type: json

  - path: cybersecurity-research/SecurityEval
    type: json

  - path: wagafo/VulQA
    type: json

  - path: Deepak/CodeRiskVulDataset
    type: json

  - path: HuggingFaceH4/cwe
    type: json

  - path: ZecOps/VulnCodeDB
    type: json

  - path: sourcegraph/security-examples
    type: json

  - path: EMBAA/openssf-scorecard-secure
    type: json

  - path: JetBrains/code_reviews
    type: json

  - path: unified/cybersecurity_qa
    type: json


  # -------------------------
  #  LOCAL DATASETS (YOURS)
  # -------------------------

  # Semgrep → your generated instructions
  - path: data/processed/semgrep_security.jsonl
    type: json
    format: alpaca

  # Semgrep synthetic → your synthetic code examples
  - path: data/processed/semgrep_synthetic.jsonl
    type: json
    format: alpaca

  # ASVS (after enrichment + conversion)
  - path: data/processed/asvs.alpaca.jsonl
    type: json
    format: alpaca


# ============================================================
# Dataset Handling
# ============================================================
dataset_prepared_path: data/processed/deepseek_dataset_cache
eval_dataset_size: 1024
sequence_len: 4096
sample_packing: true
train_on_inputs: false
group_by_length: false

# ============================================================
# Training Hyperparameters
# ============================================================
num_epochs: 1    # Increase later (1 → 2 → 3)
micro_batch_size: 1
gradient_accumulation_steps: 16
learning_rate: 2e-4
lr_scheduler: cosine
warmup_steps: 50
weight_decay: 0.01
adam_epsilon: 1e-8
max_steps: -1

# ============================================================
# Optimizations
# ============================================================
gradient_checkpointing: true
bf16: true
flash_attention: false

# ============================================================
# Output
# ============================================================
output_dir: models/secure_coder_deepseek_full
save_steps: 200
save_total_limit: 3
logging_steps: 20
wandb_project: secure-coder-deepseek-full
